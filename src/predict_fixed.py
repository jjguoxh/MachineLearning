#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆé¢„æµ‹è„šæœ¬ - è§£å†³ç‰¹å¾æ•°é‡ä¸åŒ¹é…å’Œæ— ä¿¡å·é—®é¢˜
ä¸»è¦ä¿®å¤ï¼š
1. ç¡®ä¿ä½¿ç”¨å®Œæ•´çš„88ä¸ªç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
2. é™ä½é¢„æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
3. ä¿®å¤å¼€ä»“æ— å¹³ä»“é—®é¢˜çš„è¯Šæ–­
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import sys
import glob
import warnings
from collections import Counter
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from model import TransformerClassifier, MultiScaleTransformerClassifier
from feature_engineering import add_features

# é…ç½®å‚æ•° - ä¼˜åŒ–åºåˆ—é•¿åº¦ä»¥æ•è·æ—©ç›˜æœºä¼š
SEQ_LEN = 15  # 15ä¸ªç‚¹ Ã— 4ç§’ = 60ç§’å†å²æ•°æ®ï¼Œé€‚åˆæ—©ç›˜å¿«é€Ÿååº”
MODEL_PATH = "../model/best_longtrend_model.pth"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_and_preprocess_data_fixed(csv_file):
    """
    ä¿®å¤ç‰ˆæ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼Œç¡®ä¿ä½¿ç”¨å®Œæ•´çš„88ä¸ªç‰¹å¾
    """
    print(f"ğŸ“ åŠ è½½æ•°æ®æ–‡ä»¶: {os.path.basename(csv_file)}")
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(csv_file)
    print(f"   åŸå§‹æ•°æ®ç»´åº¦: {df.shape}")
    print(f"   æ•°æ®åˆ—: {list(df.columns)}")
    
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
    required_cols = ['a', 'b', 'c', 'd', 'index_value']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
    
    # åº”ç”¨å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ï¼ˆç¡®ä¿ç”Ÿæˆ88ä¸ªç‰¹å¾ï¼‰
    print("   åº”ç”¨å®Œæ•´ç‰¹å¾å·¥ç¨‹...")
    df_with_features = add_features(df)
    print(f"   ç‰¹å¾å·¥ç¨‹åç»´åº¦: {df_with_features.shape}")
    
    # å‡†å¤‡ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ ‡ç­¾å’Œç›®æ ‡åˆ—ï¼‰
    exclude_cols = ['label', 'index_value']
    feature_cols = [col for col in df_with_features.columns if col not in exclude_cols]
    
    print(f"   âœ… ç‰¹å¾æ•°é‡: {len(feature_cols)} (æœŸæœ›: 88)")
    if len(feature_cols) != 88:
        print(f"   âš ï¸  ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼æœŸæœ›88ä¸ªï¼Œå®é™…{len(feature_cols)}ä¸ª")
        # åˆ—å‡ºç‰¹å¾ä»¥ä¾¿è°ƒè¯•
        print(f"   ç‰¹å¾åˆ—è¡¨: {feature_cols}")
    
    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    features = df_with_features[feature_cols].values
    labels = df_with_features['label'].values if 'label' in df_with_features.columns else None
    index_values = df_with_features['index_value'].values
    
    # å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
    print("   å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼...")
    features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)
    
    print(f"   âœ… æœ€ç»ˆç‰¹å¾ç»´åº¦: {features.shape}")
    
    return features, labels, index_values, feature_cols

def load_model_fixed(model_path, input_dim, num_classes=5):
    """
    ä¿®å¤ç‰ˆæ¨¡å‹åŠ è½½ï¼Œè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
    """
    print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_path}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    # æ£€æµ‹æ¨¡å‹ç±»å‹
    try:
        state_dict = torch.load(model_path, map_location='cpu', weights_only=True)
        model_keys = list(state_dict.keys())
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šå°ºåº¦æ¨¡å‹
        multiscale_keys = [key for key in model_keys if 'input_fcs' in key or 'transformer_encoders' in key]
        use_multiscale = len(multiscale_keys) > 0
        
        print(f"   æ£€æµ‹åˆ°{'å¤šå°ºåº¦' if use_multiscale else 'å•å°ºåº¦'}æ¨¡å‹")
        print(f"   æœŸæœ›è¾“å…¥ç»´åº¦: {input_dim}")
        
        # åˆ›å»ºæ¨¡å‹
        if use_multiscale:
            model = MultiScaleTransformerClassifier(
                input_dim=input_dim,
                model_dim=128,
                num_heads=8,
                num_layers=4,
                num_classes=num_classes,
                seq_lengths=[10, 30, 60],
                dropout=0.1
            ).to(DEVICE)
        else:
            model = TransformerClassifier(
                input_dim=input_dim,
                model_dim=128,
                num_heads=8,
                num_layers=4,
                num_classes=num_classes,
                dropout=0.1
            ).to(DEVICE)
        
        # åŠ è½½æƒé‡
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        
        print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, use_multiscale
        
    except Exception as e:
        print(f"   âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise e

def predict_fixed(model, X, use_multiscale=False, confidence_threshold=0.2):
    """
    ä¿®å¤ç‰ˆé¢„æµ‹ï¼Œä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼
    """
    print(f"ğŸ”® å¼€å§‹é¢„æµ‹ (ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold})")
    
    with torch.no_grad():
        if use_multiscale:
            X_tensors = {}
            for key, value in X.items():
                X_tensors[key] = torch.tensor(value, dtype=torch.float32).to(DEVICE)
            outputs = model(X_tensors)
        else:
            X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)
            outputs = model(X_tensor)
        
        # è·å–æ¦‚ç‡å’Œé¢„æµ‹
        probs = torch.nn.functional.softmax(outputs, dim=1)
        raw_preds = torch.argmax(outputs, dim=1).cpu().numpy()
        probabilities = probs.cpu().numpy()
        
        # åº”ç”¨ç½®ä¿¡åº¦è¿‡æ»¤
        predictions = raw_preds.copy()
        max_probs = np.max(probabilities, axis=1)
        low_confidence_mask = max_probs < confidence_threshold
        predictions[low_confidence_mask] = 0  # ä½ç½®ä¿¡åº¦è®¾ä¸ºæ— æ“ä½œ
        
        # ç»Ÿè®¡ä¿¡æ¯
        original_signals = np.sum(raw_preds != 0)
        filtered_signals = np.sum(predictions != 0)
        
        print(f"   åŸå§‹ä¿¡å·: {original_signals}")
        print(f"   è¿‡æ»¤åä¿¡å·: {filtered_signals}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(max_probs):.3f}")
        
        return predictions, probabilities

def create_sequences_fixed(features, seq_len=SEQ_LEN):
    """
    åˆ›å»ºåºåˆ—æ•°æ®
    """
    print(f"ğŸ“Š åˆ›å»ºåºåˆ—æ•°æ® (åºåˆ—é•¿åº¦: {seq_len})")
    
    if len(features) < seq_len:
        raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ä»¥åˆ›å»ºåºåˆ—: éœ€è¦{seq_len}ï¼Œå®é™…{len(features)}")
    
    X = []
    for i in range(len(features) - seq_len):
        X.append(features[i:i + seq_len])
    
    X = np.array(X)
    print(f"   âœ… åºåˆ—æ•°æ®ç»´åº¦: {X.shape}")
    
    return X

def analyze_predictions_fixed(predictions, probabilities, index_values):
    """
    ä¿®å¤ç‰ˆé¢„æµ‹åˆ†æï¼Œè¯¦ç»†æ£€æŸ¥å¼€ä»“å¹³ä»“åŒ¹é…
    """
    print("\n" + "="*60)
    print("ğŸ“Š é¢„æµ‹ç»“æœåˆ†æ")
    print("="*60)
    
    # 1. é¢„æµ‹åˆ†å¸ƒ
    pred_counts = Counter(predictions)
    label_names = ['æ— æ“ä½œ', 'åšå¤šå¼€ä»“', 'åšå¤šå¹³ä»“', 'åšç©ºå¼€ä»“', 'åšç©ºå¹³ä»“']
    
    print("\n1. ğŸ“ˆ é¢„æµ‹åˆ†å¸ƒ:")
    for label in range(5):
        count = pred_counts.get(label, 0)
        percentage = count / len(predictions) * 100
        print(f"   {label_names[label]} ({label}): {count} ä¸ª ({percentage:.1f}%)")
    
    # 2. ä¿¡å·å¯†åº¦
    signal_count = sum(count for label, count in pred_counts.items() if label != 0)
    signal_density = signal_count / len(predictions)
    print(f"\n2. ğŸ¯ ä¿¡å·å¯†åº¦: {signal_density:.4f} ({signal_density*100:.2f}%)")
    
    # 3. ç½®ä¿¡åº¦åˆ†æ
    max_confidences = np.max(probabilities, axis=1)
    print(f"\n3. ğŸ’ª ç½®ä¿¡åº¦åˆ†æ:")
    print(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(max_confidences):.3f}")
    print(f"   é«˜ç½®ä¿¡åº¦(>0.7): {np.sum(max_confidences > 0.7)} ä¸ª")
    print(f"   ä¸­ç½®ä¿¡åº¦(0.5-0.7): {np.sum((max_confidences >= 0.5) & (max_confidences <= 0.7))} ä¸ª")
    print(f"   ä½ç½®ä¿¡åº¦(<0.5): {np.sum(max_confidences < 0.5)} ä¸ª")
    
    # 4. å¼€ä»“å¹³ä»“åŒ¹é…æ£€æŸ¥
    print(f"\n4. ğŸ” å¼€ä»“å¹³ä»“åŒ¹é…æ£€æŸ¥:")
    long_entries = pred_counts.get(1, 0)  # åšå¤šå¼€ä»“
    long_exits = pred_counts.get(2, 0)    # åšå¤šå¹³ä»“
    short_entries = pred_counts.get(3, 0) # åšç©ºå¼€ä»“
    short_exits = pred_counts.get(4, 0)   # åšç©ºå¹³ä»“
    
    print(f"   åšå¤š: å¼€ä»“{long_entries}ä¸ª, å¹³ä»“{long_exits}ä¸ª")
    print(f"   åšç©º: å¼€ä»“{short_entries}ä¸ª, å¹³ä»“{short_exits}ä¸ª")
    
    # åŒ¹é…åº¦æ£€æŸ¥
    long_match = abs(long_entries - long_exits)
    short_match = abs(short_entries - short_exits)
    
    if long_match == 0 and short_match == 0:
        print(f"   âœ… å¼€ä»“å¹³ä»“å®Œå…¨åŒ¹é…!")
    else:
        print(f"   âš ï¸  å¼€ä»“å¹³ä»“ä¸åŒ¹é…:")
        if long_match > 0:
            print(f"     åšå¤šç¼ºå¤±: {long_match} ä¸ª{'å¹³ä»“' if long_entries > long_exits else 'å¼€ä»“'}ä¿¡å·")
        if short_match > 0:
            print(f"     åšç©ºç¼ºå¤±: {short_match} ä¸ª{'å¹³ä»“' if short_entries > short_exits else 'å¼€ä»“'}ä¿¡å·")
    
    # 5. é—®é¢˜è¯Šæ–­
    print(f"\n5. ğŸ”§ é—®é¢˜è¯Šæ–­:")
    issues = []
    
    if signal_density < 0.001:
        issues.append("ä¿¡å·å¯†åº¦æä½(<0.1%)")
    elif signal_density < 0.01:
        issues.append("ä¿¡å·å¯†åº¦åä½(<1%)")
    
    if np.mean(max_confidences) < 0.4:
        issues.append("å¹³å‡ç½®ä¿¡åº¦è¿‡ä½")
    
    if (long_entries + short_entries) == 0:
        issues.append("æ²¡æœ‰ä»»ä½•å¼€ä»“ä¿¡å·")
    
    if long_match > 0 or short_match > 0:
        issues.append("å¼€ä»“å¹³ä»“ä¿¡å·ä¸åŒ¹é…")
    
    if issues:
        print(f"   å‘ç°é—®é¢˜:")
        for issue in issues:
            print(f"     âŒ {issue}")
    else:
        print(f"   âœ… é¢„æµ‹è´¨é‡è‰¯å¥½!")
    
    return {
        'signal_density': signal_density,
        'avg_confidence': np.mean(max_confidences),
        'prediction_distribution': dict(pred_counts),
        'issues': issues
    }

def plot_results_fixed(index_values, predictions, probabilities, output_filename):
    """
    å¯è§†åŒ–é¢„æµ‹ç»“æœ
    """
    print(f"ğŸ“Š ç”Ÿæˆé¢„æµ‹å›¾è¡¨: {output_filename}")
    
    plt.figure(figsize=(15, 10))
    
    # ä¸»å›¾ï¼šä»·æ ¼æ›²çº¿å’Œä¿¡å·
    plt.subplot(3, 1, 1)
    plt.plot(index_values, label='ä»·æ ¼', alpha=0.7)
    
    # æ ‡è®°ä¿¡å·
    for i, pred in enumerate(predictions):
        if pred == 1:  # åšå¤šå¼€ä»“
            plt.scatter(i, index_values[i], color='green', marker='^', s=50, alpha=0.8)
        elif pred == 2:  # åšå¤šå¹³ä»“
            plt.scatter(i, index_values[i], color='red', marker='v', s=50, alpha=0.8)
        elif pred == 3:  # åšç©ºå¼€ä»“
            plt.scatter(i, index_values[i], color='blue', marker='v', s=50, alpha=0.8)
        elif pred == 4:  # åšç©ºå¹³ä»“
            plt.scatter(i, index_values[i], color='orange', marker='^', s=50, alpha=0.8)
    
    plt.title('ä¿®å¤ç‰ˆé¢„æµ‹ç»“æœ - ä»·æ ¼ä¸äº¤æ˜“ä¿¡å·')
    plt.legend(['ä»·æ ¼', 'åšå¤šå¼€ä»“', 'åšå¤šå¹³ä»“', 'åšç©ºå¼€ä»“', 'åšç©ºå¹³ä»“'])
    plt.grid(True, alpha=0.3)
    
    # å­å›¾ï¼šé¢„æµ‹åˆ†å¸ƒ
    plt.subplot(3, 1, 2)
    pred_counts = Counter(predictions)
    labels = ['æ— æ“ä½œ', 'åšå¤šå¼€ä»“', 'åšå¤šå¹³ä»“', 'åšç©ºå¼€ä»“', 'åšç©ºå¹³ä»“']
    values = [pred_counts.get(i, 0) for i in range(5)]
    colors = ['gray', 'green', 'red', 'blue', 'orange']
    
    bars = plt.bar(labels, values, color=colors, alpha=0.7)
    plt.title('é¢„æµ‹ä¿¡å·åˆ†å¸ƒ')
    plt.ylabel('æ•°é‡')
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, values):
        if value > 0:
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    str(value), ha='center', va='bottom')
    
    # å­å›¾ï¼šç½®ä¿¡åº¦åˆ†å¸ƒ
    plt.subplot(3, 1, 3)
    max_confidences = np.max(probabilities, axis=1)
    plt.hist(max_confidences, bins=20, alpha=0.7, color='purple')
    plt.axvline(np.mean(max_confidences), color='red', linestyle='--', 
               label=f'å¹³å‡ç½®ä¿¡åº¦: {np.mean(max_confidences):.3f}')
    plt.title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
    plt.xlabel('ç½®ä¿¡åº¦')
    plt.ylabel('é¢‘æ¬¡')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(output_filename, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ… å›¾è¡¨å·²ä¿å­˜: {output_filename}")

def main_fixed(csv_file, output_filename=None):
    """
    ä¿®å¤ç‰ˆä¸»å‡½æ•°
    """
    print(f"ğŸš€ ä¿®å¤ç‰ˆé¢„æµ‹å¼€å§‹!")
    print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {os.path.basename(csv_file)}")
    
    try:
        # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        features, labels, index_values, feature_cols = load_and_preprocess_data_fixed(csv_file)
        
        # 2. åŠ è½½æ¨¡å‹
        model, use_multiscale = load_model_fixed(MODEL_PATH, input_dim=features.shape[1])
        
        # 3. åˆ›å»ºåºåˆ—æ•°æ®
        X = create_sequences_fixed(features)
        
        # 4. è¿›è¡Œé¢„æµ‹ï¼ˆä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼‰
        predictions, probabilities = predict_fixed(model, X, use_multiscale, confidence_threshold=0.15)
        
        # 5. åˆ†æç»“æœ
        results = analyze_predictions_fixed(predictions, probabilities, index_values)
        
        # 6. å¯è§†åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if output_filename:
            plot_results_fixed(index_values, predictions, probabilities, output_filename)
        
        print(f"\nâœ… é¢„æµ‹å®Œæˆ!")
        return results
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    print("ğŸ”§ ä¿®å¤ç‰ˆé¢„æµ‹è„šæœ¬")
    print("="*50)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dirs = [
        "../data_with_relaxed_labels/",   # æœ€é«˜ä¼˜å…ˆçº§ï¼šå®½æ¾æ ‡ç­¾
        "../data_with_improved_labels/",  # ç¬¬äºŒä¼˜å…ˆçº§ï¼šæ”¹è¿›æ ‡ç­¾
        "../data_with_complete_labels/",  # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šå®Œæ•´æ ‡ç­¾
        "../data/"                        # æœ€ä½ä¼˜å…ˆçº§ï¼šåŸå§‹æ•°æ®
    ]
    
    selected_dir = None
    for data_dir in data_dirs:
        if os.path.exists(data_dir):
            csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
            if csv_files:
                selected_dir = data_dir
                print(f"ğŸ¯ ä½¿ç”¨æ•°æ®ç›®å½•: {data_dir} ({len(csv_files)}ä¸ªæ–‡ä»¶)")
                break
    
    if not selected_dir:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®ç›®å½•")
        exit(1)
    
    # æµ‹è¯•å•ä¸ªæ–‡ä»¶
    test_file = csv_files[0]
    output_file = f"./fixed_prediction_{os.path.splitext(os.path.basename(test_file))[0]}.png"
    
    print(f"\nğŸ§ª æµ‹è¯•æ–‡ä»¶: {os.path.basename(test_file)}")
    
    results = main_fixed(test_file, output_file)
    
    if results:
        print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“Š ç»“æœæ‘˜è¦:")
        print(f"   ä¿¡å·å¯†åº¦: {results['signal_density']:.4f}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {results['avg_confidence']:.3f}")
        
        if results['issues']:
            print(f"   éœ€è¦å…³æ³¨çš„é—®é¢˜: {', '.join(results['issues'])}")
        else:
            print(f"   âœ… é¢„æµ‹è´¨é‡è‰¯å¥½!")
    else:
        print(f"âŒ æµ‹è¯•å¤±è´¥")