"""
æ”¹è¿›ç‰ˆé¢„æµ‹è„šæœ¬ - é’ˆå¯¹é¢„æµ‹æ•ˆæœå·®çš„é—®é¢˜è¿›è¡Œä¼˜åŒ–
ä¸»è¦æ”¹è¿›ï¼š
1. æ›´å¥½çš„æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
2. é›†æˆå¤šç§æ¨¡å‹é¢„æµ‹
3. æ›´åˆç†çš„äº¤æ˜“ä¿¡å·è¿‡æ»¤
4. è¯¦ç»†çš„é¢„æµ‹ç»“æœåˆ†æ
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import sys
import glob
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from model import TransformerClassifier, MultiScaleTransformerClassifier
from feature_engineering import add_features

# é…ç½®å‚æ•°
SEQ_LEN = 30
MODEL_PATH = "../model/best_longtrend_model.pth"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ–°å¢ï¼šæ”¹è¿›çš„ç‰¹å¾å·¥ç¨‹
def enhanced_feature_engineering(df):
    """
    å¢å¼ºçš„ç‰¹å¾å·¥ç¨‹ï¼Œæ·»åŠ æ›´å¤šæœ‰æ•ˆç‰¹å¾
    """
    df = df.copy()
    
    # 1. åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
    for window in [5, 10, 20, 50]:
        # RSIæŒ‡æ ‡
        delta = df['index_value'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        df[f'rsi_{window}'] = 100 - (100 / (1 + rs))
        
        # å¸ƒæ—å¸¦
        sma = df['index_value'].rolling(window=window).mean()
        std = df['index_value'].rolling(window=window).std()
        df[f'bb_upper_{window}'] = sma + (std * 2)
        df[f'bb_lower_{window}'] = sma - (std * 2)
        df[f'bb_width_{window}'] = (df[f'bb_upper_{window}'] - df[f'bb_lower_{window}']) / sma
        df[f'bb_position_{window}'] = (df['index_value'] - df[f'bb_lower_{window}']) / (df[f'bb_upper_{window}'] - df[f'bb_lower_{window}'])
        
        # MACD
        if window <= 26:
            ema_fast = df['index_value'].ewm(span=12).mean()
            ema_slow = df['index_value'].ewm(span=26).mean()
            df[f'macd_{window}'] = ema_fast - ema_slow
            df[f'macd_signal_{window}'] = df[f'macd_{window}'].ewm(span=9).mean()
            df[f'macd_histogram_{window}'] = df[f'macd_{window}'] - df[f'macd_signal_{window}']
    
    # 2. ä»·æ ¼æ¨¡å¼ç‰¹å¾
    # æ”¯æ’‘é˜»åŠ›ä½
    df['resistance_distance'] = 0.0
    df['support_distance'] = 0.0
    
    lookback = 20
    for i in range(lookback, len(df)):
        recent_highs = df['index_value'].iloc[i-lookback:i].max()
        recent_lows = df['index_value'].iloc[i-lookback:i].min()
        current_price = df['index_value'].iloc[i]
        
        df.loc[i, 'resistance_distance'] = (recent_highs - current_price) / current_price
        df.loc[i, 'support_distance'] = (current_price - recent_lows) / current_price
    
    # 3. å¤šæ—¶é—´å‘¨æœŸç‰¹å¾
    for window in [3, 7, 14, 21]:
        # ä»·æ ¼åŠ¨é‡
        df[f'momentum_{window}'] = df['index_value'].pct_change(window)
        
        # ä»·æ ¼æ’åï¼ˆå½“å‰ä»·æ ¼åœ¨è¿‡å»NæœŸä¸­çš„ä½ç½®ï¼‰
        df[f'price_rank_{window}'] = df['index_value'].rolling(window=window).rank(pct=True)
        
        # æ³¢åŠ¨ç‡
        df[f'volatility_{window}'] = df['index_value'].pct_change().rolling(window=window).std()
        
        # è¶‹åŠ¿å¼ºåº¦
        df[f'trend_strength_{window}'] = (df['index_value'] - df['index_value'].shift(window)) / df['index_value'].rolling(window=window).std()
    
    # 4. æˆäº¤é‡ç›¸å…³ç‰¹å¾ï¼ˆå¦‚æœæœ‰volumeåˆ—ï¼‰
    if 'volume' in df.columns:
        for window in [5, 10, 20]:
            df[f'volume_ma_{window}'] = df['volume'].rolling(window=window).mean()
            df[f'volume_ratio_{window}'] = df['volume'] / df[f'volume_ma_{window}']
    
    # 5. å¸‚åœºç»“æ„ç‰¹å¾
    # é«˜ä½ç‚¹è¯†åˆ«
    df['local_high'] = df['index_value'].shift(1) < df['index_value']
    df['local_low'] = df['index_value'].shift(1) > df['index_value']
    
    # ä»·æ ¼ç¼ºå£
    df['gap_up'] = (df['index_value'] - df['index_value'].shift(1)) / df['index_value'].shift(1) > 0.002
    df['gap_down'] = (df['index_value'] - df['index_value'].shift(1)) / df['index_value'].shift(1) < -0.002
    
    # åº”ç”¨åŸæœ‰ç‰¹å¾å·¥ç¨‹
    df = add_features(df)
    
    # å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        # å¤„ç†æ— ç©·å¤§å€¼
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        
        # å¼‚å¸¸å€¼å¤„ç†ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df[col] = df[col].clip(lower_bound, upper_bound)
    
    # å¡«å……ç¼ºå¤±å€¼
    df.fillna(method='bfill', inplace=True)
    df.fillna(method='ffill', inplace=True)
    df.fillna(0, inplace=True)
    
    return df

def improved_data_preprocessing(df):
    """
    æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†
    """
    # å¢å¼ºç‰¹å¾å·¥ç¨‹
    df = enhanced_feature_engineering(df)
    
    # å‡†å¤‡ç‰¹å¾
    exclude_cols = ['label', 'index_value']
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    features = df[feature_cols].values
    
    # ä½¿ç”¨RobustScalerï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
    scaler = RobustScaler()
    features = scaler.fit_transform(features)
    
    return features, df['index_value'].values, feature_cols

def ensemble_predict(models, X, use_multiscale=False):
    """
    é›†æˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    """
    all_predictions = []
    all_probabilities = []
    
    for model in models:
        with torch.no_grad():
            if use_multiscale:
                X_tensors = {}
                for key, value in X.items():
                    X_tensors[key] = torch.tensor(value, dtype=torch.float32).to(DEVICE)
                outputs = model(X_tensors)
            else:
                X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)
                outputs = model(X_tensor)
            
            probs = torch.nn.functional.softmax(outputs, dim=1)
            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            
            all_predictions.append(preds)
            all_probabilities.append(probs.cpu().numpy())
    
    # æŠ•ç¥¨æ³•é›†æˆé¢„æµ‹
    ensemble_predictions = []
    ensemble_probabilities = []
    
    for i in range(len(all_predictions[0])):
        # é¢„æµ‹æŠ•ç¥¨
        votes = [pred[i] for pred in all_predictions]
        ensemble_pred = max(set(votes), key=votes.count)
        ensemble_predictions.append(ensemble_pred)
        
        # æ¦‚ç‡å¹³å‡
        avg_prob = np.mean([prob[i] for prob in all_probabilities], axis=0)
        ensemble_probabilities.append(avg_prob)
    
    return np.array(ensemble_predictions), np.array(ensemble_probabilities)

def improved_signal_filtering(predictions, probabilities, index_values, confidence_threshold=0.3):
    """
    æ”¹è¿›çš„ä¿¡å·è¿‡æ»¤ï¼Œå‡å°‘è™šå‡ä¿¡å·
    é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ä»¥å¢åŠ ä¿¡å·å¯†åº¦
    """
    filtered_predictions = predictions.copy()
    
    print(f"ğŸ“Š åº”ç”¨ä¿¡å·è¿‡æ»¤ - ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
    original_signal_count = np.sum(predictions != 0)
    
    for i in range(len(predictions)):
        # 1. ç½®ä¿¡åº¦è¿‡æ»¤ - é™ä½é˜ˆå€¼
        max_prob = np.max(probabilities[i])
        if max_prob < confidence_threshold:
            filtered_predictions[i] = 0  # è®¾ä¸ºæ— æ“ä½œ
        
        # 2. è¿ç»­ä¿¡å·è¿‡æ»¤ï¼ˆé¿å…é¢‘ç¹äº¤æ˜“ï¼‰- æ”¾å®½æ¡ä»¶
        if i > 2 and predictions[i] == predictions[i-1] == predictions[i-2] and predictions[i] != 0:
            filtered_predictions[i] = 0
        
        # 3. ä»·æ ¼ç¡®è®¤è¿‡æ»¤ - æ”¾å®½æ¡ä»¶
        if i >= 3:  # å‡å°‘éœ€è¦çš„å†å²æ•°æ®
            recent_trend = (index_values[i] - index_values[i-3]) / index_values[i-3]
            
            # æ”¾å®½ä»·æ ¼ç¡®è®¤æ¡ä»¶
            if (predictions[i] == 1 and recent_trend < -0.003) or \
               (predictions[i] == 3 and recent_trend > 0.003):
                # åªåœ¨è¶‹åŠ¿æ˜æ˜¾ç›¸åæ—¶æ‰è¿‡æ»¤
                filtered_predictions[i] = 0
    
    filtered_signal_count = np.sum(filtered_predictions != 0)
    filter_ratio = filtered_signal_count / original_signal_count if original_signal_count > 0 else 0
    
    print(f"ğŸ“ˆ ä¿¡å·è¿‡æ»¤ç»“æœ: {original_signal_count} -> {filtered_signal_count} (ä¿ç•™ç‡: {filter_ratio:.1%})")
    
    return filtered_predictions

def detailed_performance_analysis(predictions, probabilities, index_values):
    """
    è¯¦ç»†çš„é¢„æµ‹æ€§èƒ½åˆ†æ - å¢åŠ å¼€ä»“å¹³ä»“åŒ¹é…æ£€æŸ¥
    """
    print("\n" + "="*60)
    print("è¯¦ç»†é¢„æµ‹æ€§èƒ½åˆ†æ")
    print("="*60)
    
    # 1. é¢„æµ‹åˆ†å¸ƒåˆ†æ
    unique, counts = np.unique(predictions, return_counts=True)
    label_names = ['æ— æ“ä½œ', 'åšå¤šå¼€ä»“', 'åšå¤šå¹³ä»“', 'åšç©ºå¼€ä»“', 'åšç©ºå¹³ä»“']
    
    print("\n1. é¢„æµ‹åˆ†å¸ƒ:")
    for label, count in zip(unique, counts):
        percentage = count / len(predictions) * 100
        print(f"   {label_names[label]} ({label}): {count} ä¸ª ({percentage:.1f}%)")
    
    # 2. ç½®ä¿¡åº¦åˆ†æ
    print("\n2. ç½®ä¿¡åº¦åˆ†æ:")
    max_confidences = np.max(probabilities, axis=1)
    print(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(max_confidences):.3f}")
    print(f"   ç½®ä¿¡åº¦æ ‡å‡†å·®: {np.std(max_confidences):.3f}")
    print(f"   é«˜ç½®ä¿¡åº¦é¢„æµ‹(>0.7): {np.sum(max_confidences > 0.7)} ä¸ª")
    print(f"   ä½ç½®ä¿¡åº¦é¢„æµ‹(<0.5): {np.sum(max_confidences < 0.5)} ä¸ª")
    
    # 3. ä¿¡å·å¯†åº¦åˆ†æ
    non_zero_signals = np.sum(predictions != 0)
    signal_density = non_zero_signals / len(predictions)
    print(f"\n3. ä¿¡å·å¯†åº¦: {signal_density:.3f} ({non_zero_signals}/{len(predictions)})")
    
    # 4. ç®€å•å›æµ‹åˆ†æï¼ˆåŒ…å«å¼€ä»“å¹³ä»“åŒ¹é…æ£€æŸ¥ï¼‰
    print("\n4. ç®€å•å›æµ‹åˆ†æ:")
    analyze_simple_backtest(predictions, index_values)
    
    return {
        'prediction_distribution': dict(zip(unique, counts)),
        'avg_confidence': np.mean(max_confidences),
        'signal_density': signal_density
    }

def analyze_simple_backtest(predictions, index_values):
    """
    ç®€å•çš„å›æµ‹åˆ†æ - ä¿®å¤å¼€ä»“æ— å¹³ä»“çš„é—®é¢˜
    """
    position = 0  # 0: æ— ä»“ä½, 1: å¤šå¤´, -1: ç©ºå¤´
    entry_price = 0
    trades = []
    equity_curve = [1.0]  # ä»1å¼€å§‹çš„æƒç›Šæ›²çº¿
    
    # è®°å½•æœªå¹³ä»“çš„å¼€ä»“ä¿¡å·
    unmatched_long_entries = 0
    unmatched_short_entries = 0
    
    for i in range(len(predictions)):
        pred = predictions[i]
        current_price = index_values[i] if i < len(index_values) else index_values[-1]
        
        # å¼€ä»“ä¿¡å·
        if pred == 1 and position == 0:  # åšå¤šå¼€ä»“
            position = 1
            entry_price = current_price
        elif pred == 3 and position == 0:  # åšç©ºå¼€ä»“
            position = -1
            entry_price = current_price
        
        # å¹³ä»“ä¿¡å·
        elif pred == 2 and position == 1:  # åšå¤šå¹³ä»“
            profit = (current_price - entry_price) / entry_price
            trades.append(profit)
            position = 0
        elif pred == 4 and position == -1:  # åšç©ºå¹³ä»“
            profit = (entry_price - current_price) / entry_price
            trades.append(profit)
            position = 0
        
        # è®¡ç®—æœªåŒ¹é…çš„ä¿¡å·
        elif pred == 1 and position != 0:  # å¼€ä»“ä¿¡å·ä½†å·²æœ‰ä»“ä½
            unmatched_long_entries += 1
        elif pred == 3 and position != 0:
            unmatched_short_entries += 1
        elif pred == 2 and position != 1:  # å¹³ä»“ä¿¡å·ä½†æ— å¯¹åº”ä»“ä½
            pass  # æ— æ³•å¹³ä»“
        elif pred == 4 and position != -1:
            pass  # æ— æ³•å¹³ä»“
        
        # æ›´æ–°æƒç›Šæ›²çº¿
        if len(trades) > 0:
            total_return = np.prod([1 + t for t in trades])
            equity_curve.append(total_return)
        else:
            equity_curve.append(equity_curve[-1])
    
    # æ£€æŸ¥æœªå¹³ä»“çš„äº¤æ˜“
    open_position_warning = ""
    if position != 0:
        pos_type = "å¤šå¤´" if position == 1 else "ç©ºå¤´"
        open_position_warning = f"\n   âš ï¸  æœ€åä»æœ‰æœªå¹³ä»“çš„{pos_type}ä»“ä½ï¼"
    
    if trades:
        win_rate = np.sum(np.array(trades) > 0) / len(trades)
        avg_profit = np.mean(trades)
        max_profit = np.max(trades)
        max_loss = np.min(trades)
        total_return = np.prod([1 + t for t in trades]) - 1
        
        print(f"   æ€»äº¤æ˜“æ¬¡æ•°: {len(trades)}")
        print(f"   èƒœç‡: {win_rate:.2%}")
        print(f"   å¹³å‡æ”¶ç›Š: {avg_profit:.4f} ({avg_profit*100:.2f}%)")
        print(f"   æœ€å¤§ç›ˆåˆ©: {max_profit:.4f} ({max_profit*100:.2f}%)")
        print(f"   æœ€å¤§äºæŸ: {max_loss:.4f} ({max_loss*100:.2f}%)")
        print(f"   æ€»æ”¶ç›Šç‡: {total_return:.4f} ({total_return*100:.2f}%)")
        
        # æ˜¾ç¤ºé—®é¢˜è¯Šæ–­
        if unmatched_long_entries > 0 or unmatched_short_entries > 0:
            print(f"   âš ï¸  å‘ç°æ ‡ç­¾é—®é¢˜:")
            if unmatched_long_entries > 0:
                print(f"     æ— æ³•åŒ¹é…çš„åšå¤šå¼€ä»“: {unmatched_long_entries} ä¸ª")
            if unmatched_short_entries > 0:
                print(f"     æ— æ³•åŒ¹é…çš„åšç©ºå¼€ä»“: {unmatched_short_entries} ä¸ª")
            print(f"     ğŸ’¡ å»ºè®®ä½¿ç”¨ generate_complete_trading_labels.py ç”Ÿæˆå®Œæ•´äº¤æ˜“æ ‡ç­¾")
        
        print(open_position_warning)
        
    else:
        print("   æ²¡æœ‰å®Œæˆçš„äº¤æ˜“")
        if unmatched_long_entries > 0 or unmatched_short_entries > 0:
            print(f"   âš ï¸  ä½†å‘ç°æœªåŒ¹é…çš„å¼€ä»“ä¿¡å·:")
            print(f"     åšå¤šå¼€ä»“: {unmatched_long_entries} ä¸ª")
            print(f"     åšç©ºå¼€ä»“: {unmatched_short_entries} ä¸ª")
            print(f"     ğŸ’¡ è¯´æ˜æ ‡ç­¾ç”Ÿæˆæœ‰é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨ generate_complete_trading_labels.py")

def plot_improved_signals(index_values, predictions, probabilities, output_filename=None, use_multiscale=False):
    """
    æ”¹è¿›çš„ä¿¡å·ç»˜åˆ¶ï¼ŒåŒ…å«æ›´å¤šåˆ†æä¿¡æ¯
    """
    # åˆ›å»ºå­å›¾
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))
    
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦æ¨¡å‹è°ƒæ•´ç´¢å¼•
    start_idx = 60 if use_multiscale else SEQ_LEN
    
    # å­å›¾1: ä»·æ ¼å’Œäº¤æ˜“ä¿¡å·
    ax1.plot(index_values, label='Price', color='blue', linewidth=1)
    
    # ç»˜åˆ¶äº¤æ˜“ä¿¡å·
    signal_colors = {1: 'red', 2: 'red', 3: 'green', 4: 'green'}
    signal_markers = {1: '^', 2: 'v', 3: '^', 4: 'v'}
    signal_labels = {1: 'Long Entry', 2: 'Long Exit', 3: 'Short Entry', 4: 'Short Exit'}
    
    for pred_type in [1, 2, 3, 4]:
        indices = []
        prices = []
        for i, pred in enumerate(predictions):
            if pred == pred_type:
                idx = i + start_idx
                if idx < len(index_values):
                    indices.append(idx)
                    prices.append(index_values[idx])
        
        if indices:
            ax1.scatter(indices, prices, color=signal_colors[pred_type], 
                       marker=signal_markers[pred_type], s=60, 
                       label=signal_labels[pred_type], zorder=5, alpha=0.8)
    
    ax1.set_title('ä»·æ ¼å›¾è¡¨ä¸äº¤æ˜“ä¿¡å·')
    ax1.set_ylabel('ä»·æ ¼')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å­å›¾2: é¢„æµ‹ç½®ä¿¡åº¦
    max_confidences = np.max(probabilities, axis=1)
    x_range = range(start_idx, start_idx + len(max_confidences))
    ax2.plot(x_range, max_confidences, color='purple', alpha=0.7)
    ax2.axhline(y=0.6, color='red', linestyle='--', alpha=0.5, label='ç½®ä¿¡åº¦é˜ˆå€¼')
    ax2.fill_between(x_range, max_confidences, alpha=0.3, color='purple')
    ax2.set_title('é¢„æµ‹ç½®ä¿¡åº¦')
    ax2.set_ylabel('ç½®ä¿¡åº¦')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # å­å›¾3: é¢„æµ‹åˆ†å¸ƒ
    pred_counts = np.bincount(predictions, minlength=5)
    label_names = ['æ— æ“ä½œ', 'åšå¤šå¼€ä»“', 'åšå¤šå¹³ä»“', 'åšç©ºå¼€ä»“', 'åšç©ºå¹³ä»“']
    colors = ['gray', 'red', 'red', 'green', 'green']
    
    bars = ax3.bar(range(5), pred_counts, color=colors, alpha=0.7)
    ax3.set_title('é¢„æµ‹åˆ†å¸ƒç»Ÿè®¡')
    ax3.set_xlabel('é¢„æµ‹ç±»å‹')
    ax3.set_ylabel('æ•°é‡')
    ax3.set_xticks(range(5))
    ax3.set_xticklabels(label_names, rotation=45)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for i, bar in enumerate(bars):
        height = bar.get_height()
        if height > 0:
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{int(height)}', ha='center', va='bottom')
    
    plt.tight_layout()
    
    # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
    if output_filename:
        plt.savefig(output_filename, dpi=300, bbox_inches='tight')
        print(f"æ”¹è¿›ç‰ˆå›¾è¡¨å·²ä¿å­˜ä¸º: {output_filename}")
        plt.close()
    else:
        plt.show()

def main_improved(data_file, use_multiscale=None, output_filename=None):
    """
    æ”¹è¿›ç‰ˆä¸»å‡½æ•°
    """
    print("å¼€å§‹æ”¹è¿›ç‰ˆé¢„æµ‹...")
    
    # è¯»å–æ•°æ®
    print(f"è¯»å–æ•°æ®æ–‡ä»¶: {data_file}")
    df = pd.read_csv(data_file)
    print(f"æ•°æ®æ¡æ•°: {len(df)}")
    
    # æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†
    print("è¿›è¡Œæ”¹è¿›çš„æ•°æ®é¢„å¤„ç†...")
    features, index_values, feature_cols = improved_data_preprocessing(df)
    print(f"ç‰¹å¾ç»´åº¦: {features.shape}")
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    
    # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ç±»å‹ï¼Œåˆ™å°è¯•è‡ªåŠ¨æ£€æµ‹
    if use_multiscale is None:
        print("è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹...")
        use_multiscale = detect_model_type(MODEL_PATH)
    
    # åˆ›å»ºåºåˆ—
    print("åˆ›å»ºåºåˆ—æ•°æ®...")
    if use_multiscale:
        X = create_multiscale_sequences_for_prediction(features, seq_lengths=[10, 30, 60])
        num_classes = 5
        print("ä½¿ç”¨å¤šå°ºåº¦æ¨¡å‹")
    else:
        X = create_sequences_for_prediction(features, seq_len=SEQ_LEN)
        num_classes = 5
        print("ä½¿ç”¨å•å°ºåº¦æ¨¡å‹")
    
    if len(X) == 0:
        print("åºåˆ—æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
        return
    
    print(f"åºåˆ—æ•°é‡: {len(X)}")
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    try:
        model = load_model(MODEL_PATH, features.shape[1], num_classes, use_multiscale, 
                          [10, 30, 60] if use_multiscale else None)
        models = [model]  # å¯ä»¥æ‰©å±•ä¸ºå¤šä¸ªæ¨¡å‹
        print("æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # è¿›è¡Œé¢„æµ‹
    print("è¿›è¡Œé›†æˆé¢„æµ‹...")
    if len(models) == 1:
        predictions, probabilities = predict(models[0], X, use_multiscale)
    else:
        predictions, probabilities = ensemble_predict(models, X, use_multiscale)
    
    print(f"é¢„æµ‹å®Œæˆï¼Œå…± {len(predictions)} ä¸ªé¢„æµ‹ç»“æœ")
    
    # æ”¹è¿›çš„ä¿¡å·è¿‡æ»¤
    print("åº”ç”¨æ”¹è¿›çš„ä¿¡å·è¿‡æ»¤...")
    original_signals = np.sum(predictions != 0)
    
    # å¦‚æœåŸå§‹ä¿¡å·å°±å¾ˆå°‘ï¼Œä½¿ç”¨æ›´å®½æ¾çš„è¿‡æ»¤
    if original_signals / len(predictions) < 0.01:  # å¦‚æœä¿¡å·å¯†åº¦<1%
        print("âš ï¸  æ£€æµ‹åˆ°ä¿¡å·å¯†åº¦è¿‡ä½ï¼Œä½¿ç”¨å®½æ¾è¿‡æ»¤ç­–ç•¥")
        predictions = improved_signal_filtering(predictions, probabilities, index_values, confidence_threshold=0.25)
    elif original_signals / len(predictions) < 0.03:  # å¦‚æœä¿¡å·å¯†åº¦<3%
        print("ğŸ“Š æ£€æµ‹åˆ°ä¿¡å·å¯†åº¦è¾ƒä½ï¼Œä½¿ç”¨ä¸­ç­‰è¿‡æ»¤ç­–ç•¥")
        predictions = improved_signal_filtering(predictions, probabilities, index_values, confidence_threshold=0.35)
    else:
        predictions = improved_signal_filtering(predictions, probabilities, index_values, confidence_threshold=0.45)
    
    filtered_signals = np.sum(predictions != 0)
    print(f"ä¿¡å·è¿‡æ»¤: {original_signals} -> {filtered_signals} ({filtered_signals/original_signals:.2%} ä¿ç•™)" if original_signals > 0 else "ä¿¡å·è¿‡æ»¤: æ— åŸå§‹ä¿¡å·")
    
    # è¯¦ç»†æ€§èƒ½åˆ†æ
    analysis_results = detailed_performance_analysis(predictions, probabilities, index_values)
    
    # å¯è§†åŒ–ç»“æœ
    print("ç”Ÿæˆæ”¹è¿›ç‰ˆå›¾è¡¨...")
    plot_improved_signals(index_values, predictions, probabilities, output_filename, use_multiscale)
    
    print("æ”¹è¿›ç‰ˆé¢„æµ‹å’Œåˆ†æå®Œæˆ!")
    return analysis_results

# å¯¼å…¥åŸæœ‰å‡½æ•°
def load_model(model_path, input_dim, num_classes, use_multiscale=False, seq_lengths=[10, 30, 60]):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if use_multiscale:
        model = MultiScaleTransformerClassifier(
            input_dim=input_dim,
            model_dim=128,
            num_heads=8,
            num_layers=4,
            num_classes=num_classes,
            seq_lengths=seq_lengths,
            dropout=0.1
        ).to(DEVICE)
    else:
        model = TransformerClassifier(
            input_dim=input_dim,
            model_dim=128,
            num_heads=8,
            num_layers=4,
            num_classes=num_classes,
            dropout=0.1
        ).to(DEVICE)
    
    # å®‰å…¨åŠ è½½æ¨¡å‹æƒé‡
    try:
        model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))
    except RuntimeError as e:
        if "Missing key" in str(e) or "Unexpected key" in str(e):
            print("è­¦å‘Š: æ¨¡å‹ç»“æ„ä¸æƒé‡ä¸åŒ¹é…ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤...")
            state_dict = torch.load(model_path, map_location=DEVICE, weights_only=True)
            model.load_state_dict(state_dict, strict=False)
        else:
            raise e
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹æƒé‡æ—¶å‡ºé”™: {e}")
        raise e
        
    model.eval()
    return model

def create_sequences_for_prediction(features, seq_len=SEQ_LEN):
    """ä¸ºé¢„æµ‹åˆ›å»ºåºåˆ—æ•°æ®"""
    X = []
    for i in range(len(features) - seq_len):
        X.append(features[i:i + seq_len])
    return np.array(X)

def create_multiscale_sequences_for_prediction(features, seq_lengths=[10, 30, 60]):
    """ä¸ºå¤šå°ºåº¦æ¨¡å‹åˆ›å»ºåºåˆ—æ•°æ®"""
    X_multi = {str(length): [] for length in seq_lengths}
    max_len = max(seq_lengths)
    
    for i in range(len(features) - max_len):
        for seq_len in seq_lengths:
            X_multi[str(seq_len)].append(features[i:i + seq_len])
    
    for seq_len in seq_lengths:
        X_multi[str(seq_len)] = np.array(X_multi[str(seq_len)])
    return X_multi

def predict(model, X, use_multiscale=False):
    """ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    with torch.no_grad():
        if use_multiscale:
            X_tensors = {}
            for key, value in X.items():
                X_tensors[key] = torch.tensor(value, dtype=torch.float32).to(DEVICE)
            outputs = model(X_tensors)
        else:
            X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)
            outputs = model(X_tensor)
        
        probs = torch.nn.functional.softmax(outputs, dim=1)
        preds = torch.argmax(outputs, dim=1).cpu().numpy()
        return preds, probs.cpu().numpy()

def detect_model_type(model_path):
    """å°è¯•æ£€æµ‹æ¨¡å‹ç±»å‹ï¼ˆå•å°ºåº¦æˆ–å¤šå°ºåº¦ï¼‰"""
    try:
        state_dict = torch.load(model_path, map_location='cpu', weights_only=True)
        multiscale_keys = [key for key in state_dict.keys() if 'input_fcs' in key or 'transformer_encoders' in key]
        if multiscale_keys:
            print("æ£€æµ‹åˆ°å¤šå°ºåº¦æ¨¡å‹æƒé‡")
            return True
        else:
            print("æ£€æµ‹åˆ°å•å°ºåº¦æ¨¡å‹æƒé‡")
            return False
    except Exception as e:
        print(f"æ£€æµ‹æ¨¡å‹ç±»å‹æ—¶å‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    # æ‰¹é‡å¤„ç†../data/ç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
    # ä¼˜å…ˆçº§ï¼šå®½æ¾æ ‡ç­¾ > æ”¹è¿›æ ‡ç­¾ > å®Œæ•´äº¤æ˜“æ ‡ç­¾ > åŸå§‹æ•°æ®
    relaxed_labels_dir = "../data_with_relaxed_labels/"
    improved_data_dir = "../data_with_improved_labels/"
    complete_labels_dir = "../data_with_complete_labels/"
    original_data_dir = "../data/"
    
    # æ£€æŸ¥æ•°æ®ç›®å½•ä¼˜å…ˆçº§
    if os.path.exists(relaxed_labels_dir):
        data_dir = relaxed_labels_dir
        data_type = "relaxed"
        print(f"ğŸ† ä½¿ç”¨å®½æ¾å‚æ•°æ ‡ç­¾æ•°æ®ç›®å½•: {data_dir}")
        print(f"ğŸ“Š æœŸæœ›ä¿¡å·å¯†åº¦: 2.5-3.1%, èƒœç‡: 100%")
    elif os.path.exists(improved_data_dir):
        data_dir = improved_data_dir
        data_type = "improved"
        print(f"ğŸ”„ ä½¿ç”¨æ”¹è¿›æ ‡ç­¾æ•°æ®ç›®å½•: {data_dir}")
        print(f"ğŸ“Š æœŸæœ›ä¿¡å·å¯†åº¦: ~37.8%")
    elif os.path.exists(complete_labels_dir):
        data_dir = complete_labels_dir
        data_type = "complete"
        print(f"âœ… ä½¿ç”¨å®Œæ•´äº¤æ˜“æ ‡ç­¾æ•°æ®ç›®å½•: {data_dir}")
    else:
        data_dir = original_data_dir
        data_type = "original"
        print(f"âš ï¸  ä½¿ç”¨åŸå§‹æ•°æ®ç›®å½•: {data_dir}")
        print(f"ğŸ’¡ æç¤º: è¿è¡Œ diagnose_signal_density.py ç”Ÿæˆæ”¹è¿›æ ‡ç­¾")
    
    use_multiscale = True  # æ˜¾å¼è®¾ç½®ä¸ºå¤šå°ºåº¦
    
    if not os.path.exists(data_dir):
        print(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        sys.exit(1)
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    
    if not csv_files:
        print(f"åœ¨ç›®å½• {data_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        sys.exit(1)
    
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if data_type == "relaxed":
        output_dir = "../predictions_with_relaxed_labels/"
    elif data_type == "improved":
        output_dir = "../predictions_with_improved_labels/"
    elif data_type == "complete":
        output_dir = "../predictions_with_complete_labels/"
    else:
        output_dir = "../predictions_improved/"
    os.makedirs(output_dir, exist_ok=True)
    
    # å¤„ç†æ¯ä¸ªCSVæ–‡ä»¶
    all_results = {}
    for csv_file in csv_files:
        print(f"\n{'='*80}")
        print(f"å¤„ç†æ–‡ä»¶: {csv_file}")
        
        # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        base_name = os.path.splitext(os.path.basename(csv_file))[0]
        if data_type == "relaxed":
            output_filename = os.path.join(output_dir, f"{base_name}_relaxed_labels.png")
        elif data_type == "improved":
            output_filename = os.path.join(output_dir, f"{base_name}_improved_labels.png")
        elif data_type == "complete":
            output_filename = os.path.join(output_dir, f"{base_name}_complete_labels.png")
        else:
            output_filename = os.path.join(output_dir, f"{base_name}_improved_prediction.png")
        
        try:
            results = main_improved(csv_file, use_multiscale, output_filename)
            all_results[base_name] = results
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {csv_file} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    # æ€»ç»“æ‰€æœ‰æ–‡ä»¶çš„ç»“æœ
    print(f"\n{'='*80}")
    print("æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼æ€»ç»“æŠ¥å‘Š:")
    print(f"æ”¹è¿›ç‰ˆé¢„æµ‹ç»“æœå›¾ç‰‡ä¿å­˜åœ¨: {output_dir}")
    
    # ç»™å‡ºè¿›ä¸€æ­¥æ”¹è¿›å»ºè®®
    if data_type == "relaxed":
        if all_results and np.mean([r['signal_density'] for r in all_results.values()]) > 0.02:
            print(f"\nğŸ† å®½æ¾æ ‡ç­¾æ•ˆæœå¾ˆå¥½ï¼")
            print(f"   å»ºè®®ä¸‹ä¸€æ­¥:")
            print(f"   1. ä½¿ç”¨å½“å‰æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹")
            print(f"   2. è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹")
            print(f"   3. éªŒè¯å®é™…äº¤æ˜“æ•ˆæœ")
        else:
            print(f"\nğŸ”§ å®½æ¾æ ‡ç­¾ä»éœ€è°ƒæ•´:")
            print(f"   1. è¿›ä¸€æ­¥é™ä½ min_profit_target")
            print(f"   2. å‡å°‘ min_hold_time")
            print(f"   3. ç¼©å° min_signal_gap")
    elif data_type == "original":
        print(f"\nğŸ’¡ å»ºè®®çš„ä¸‹ä¸€æ­¥æ”¹è¿›:")
        print(f"   1. è¿è¡Œ 'python diagnose_signal_density.py' ç”Ÿæˆæ”¹è¿›æ ‡ç­¾")
        print(f"   2. ä½¿ç”¨æ–°æ ‡ç­¾é‡æ–°è®­ç»ƒæ¨¡å‹")
        print(f"   3. å†æ¬¡è¿è¡Œæ­¤é¢„æµ‹è„šæœ¬æŸ¥çœ‹æ•ˆæœ")
    elif data_type == "improved":
        print(f"\nğŸ“Š æ”¹è¿›æ ‡ç­¾æ•ˆæœåˆ†æ:")
        if all_results:
            avg_signal_density = np.mean([r['signal_density'] for r in all_results.values()])
            if avg_signal_density > 0.3:
                print(f"   âœ… ä¿¡å·å¯†åº¦å¾ˆé«˜: {avg_signal_density:.1%}")
                print(f"   å»ºè®®: æ£€æŸ¥äº¤æ˜“é¢‘ç‡æ˜¯å¦è¿‡é«˜ï¼Œè€ƒè™‘ä½¿ç”¨å®Œæ•´äº¤æ˜“æ ‡ç­¾")
            else:
                print(f"   ä¿¡å·å¯†åº¦: {avg_signal_density:.1%}")
    else:
        if all_results and np.mean([r['signal_density'] for r in all_results.values()]) < 0.01:
            print(f"\nğŸš¨ ä¿¡å·å¯†åº¦ä»ç„¶è¿‡ä½çš„è§£å†³å»ºè®®:")
            print(f"   1. æ£€æŸ¥æ¨¡å‹è®­ç»ƒæ•°æ®è´¨é‡")
            print(f"   2. è¿è¡Œ diagnose_signal_density.py ç”Ÿæˆå®½æ¾æ ‡ç­¾")
            print(f"   3. é‡æ–°è®­ç»ƒæ¨¡å‹")
        else:
            print(f"\nğŸ¯ å¦‚æœé¢„æµ‹æ•ˆæœä»ä¸ç†æƒ³ï¼Œå¯ä»¥å°è¯•:")
            print(f"   1. è°ƒæ•´ generate_complete_trading_labels.py ä¸­çš„å‚æ•°")
            print(f"   2. é‡æ–°è®­ç»ƒæ¨¡å‹")
            print(f"   3. ä½¿ç”¨é›†æˆå­¦ä¹ æ–¹æ³•")
    
    if all_results:
        avg_confidence = np.mean([r['avg_confidence'] for r in all_results.values()])
        if avg_confidence < 0.4:
            print(f"å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦è¾ƒä½: {avg_confidence:.3f}")
        avg_signal_density = np.mean([r['signal_density'] for r in all_results.values()])
        print(f"å¹³å‡ä¿¡å·å¯†åº¦: {avg_signal_density:.3f}")
        
        if avg_signal_density < 0.001:
            print(f"\nğŸš¨ ä¸¥é‡é—®é¢˜: å¹³å‡ä¿¡å·å¯†åº¦ {avg_signal_density:.3f} æä½!")
            print(f"å»ºè®®ç«‹å³:")
            print(f"  1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½")
            print(f"  2. éªŒè¯è®­ç»ƒæ•°æ®æ ‡ç­¾åˆ†å¸ƒ")
            print(f"  3. é‡æ–°ç”Ÿæˆæ ‡ç­¾å¹¶è®­ç»ƒæ¨¡å‹")