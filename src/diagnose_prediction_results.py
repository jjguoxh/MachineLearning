#!/usr/bin/env python3
"""
é¢„æµ‹ç»“æœè¯Šæ–­å·¥å…·
åˆ†æä¸ºä»€ä¹ˆé¢„æµ‹æ–‡ä»¶ä¸­æ²¡æœ‰å¼€ä»“å’Œå¹³ä»“ä¿¡å·
"""

import pandas as pd
import numpy as np
import os
import glob
import torch
import torch.nn as nn
from collections import Counter
import matplotlib.pyplot as plt

def diagnose_prediction_issue():
    """
    è¯Šæ–­é¢„æµ‹ç»“æœé—®é¢˜
    """
    print("ğŸ” é¢„æµ‹ç»“æœé—®é¢˜è¯Šæ–­å·¥å…·")
    print("="*60)
    
    # 1. æ£€æŸ¥æ•°æ®ç›®å½•å’Œæ–‡ä»¶
    print("\nğŸ“ æ£€æŸ¥æ•°æ®ç›®å½•å’Œæ–‡ä»¶...")
    
    relaxed_dir = "./data_with_relaxed_labels/"
    model_path = "./model/best_longtrend_model.pth"
    
    # æ£€æŸ¥å®½æ¾æ ‡ç­¾æ•°æ®
    if os.path.exists(relaxed_dir):
        csv_files = glob.glob(os.path.join(relaxed_dir, "*.csv"))
        print(f"âœ… å®½æ¾æ ‡ç­¾æ•°æ®ç›®å½•å­˜åœ¨: {len(csv_files)} ä¸ªæ–‡ä»¶")
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        sample_file = csv_files[0] if csv_files else None
        if sample_file:
            df = pd.read_csv(sample_file)
            if 'label' in df.columns:
                labels = df['label'].values
                label_counts = Counter(labels)
                signal_density = sum(count for label, count in label_counts.items() if label != 0) / len(labels)
                
                print(f"   æ ·æœ¬æ–‡ä»¶: {os.path.basename(sample_file)}")
                print(f"   æ ‡ç­¾åˆ†å¸ƒ: {dict(label_counts)}")
                print(f"   ä¿¡å·å¯†åº¦: {signal_density:.4f} ({signal_density*100:.2f}%)")
                
                if signal_density > 0.02:
                    print(f"   âœ… è®­ç»ƒæ•°æ®æ ‡ç­¾è´¨é‡è‰¯å¥½")
                    data_quality_ok = True
                else:
                    print(f"   âš ï¸  è®­ç»ƒæ•°æ®ä¿¡å·å¯†åº¦åä½")
                    data_quality_ok = False
            else:
                print(f"   âŒ æ ·æœ¬æ–‡ä»¶ç¼ºå°‘labelåˆ—")
                data_quality_ok = False
        else:
            print(f"   âŒ æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
            data_quality_ok = False
    else:
        print(f"âŒ å®½æ¾æ ‡ç­¾æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        data_quality_ok = False
    
    # 2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    print(f"\nğŸ¤– æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    if os.path.exists(model_path):
        print(f"âœ… æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {model_path}")
        
        # å°è¯•åŠ è½½æ¨¡å‹æƒé‡æŸ¥çœ‹ä¿¡æ¯
        try:
            state_dict = torch.load(model_path, map_location='cpu', weights_only=True)
            model_keys = list(state_dict.keys())
            print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {len(model_keys)}")
            print(f"   å‰å‡ ä¸ªå‚æ•°å: {model_keys[:3]}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šå°ºåº¦æ¨¡å‹
            multiscale_keys = [key for key in model_keys if 'input_fcs' in key or 'transformer_encoders' in key]
            if multiscale_keys:
                print(f"   âœ… æ£€æµ‹åˆ°å¤šå°ºåº¦æ¨¡å‹")
                model_type = "multiscale"
            else:
                print(f"   ğŸ“Š æ£€æµ‹åˆ°å•å°ºåº¦æ¨¡å‹")
                model_type = "single"
                
            model_ok = True
        except Exception as e:
            print(f"   âŒ æ¨¡å‹æ–‡ä»¶æŸå: {e}")
            model_ok = False
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        model_ok = False
    
    # 3. ç®€å•é¢„æµ‹æµ‹è¯•
    print(f"\nğŸ§ª è¿›è¡Œç®€å•é¢„æµ‹æµ‹è¯•...")
    if data_quality_ok and model_ok and csv_files:
        try:
            # å¯¼å…¥å¿…è¦çš„æ¨¡å‹ç±»
            from model import TransformerClassifier, MultiScaleTransformerClassifier
            
            # è¯»å–æµ‹è¯•æ•°æ®
            test_file = csv_files[0]
            df = pd.read_csv(test_file)
            
            print(f"   æµ‹è¯•æ–‡ä»¶: {os.path.basename(test_file)}")
            print(f"   æ•°æ®è¡Œæ•°: {len(df)}")
            
            # æ£€æŸ¥æ•°æ®åˆ—
            required_cols = ['a', 'b', 'c', 'd', 'index_value']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"   âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                return
            
            # å‡†å¤‡ç‰¹å¾æ•°æ®
            features = df[['a', 'b', 'c', 'd', 'index_value']].values
            print(f"   ç‰¹å¾ç»´åº¦: {features.shape}")
            
            # åˆ›å»ºåºåˆ—æ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            seq_len = 60
            if len(features) > seq_len:
                X = []
                for i in range(len(features) - seq_len):
                    X.append(features[i:i + seq_len])
                X = np.array(X)
                print(f"   åºåˆ—æ•°æ®ç»´åº¦: {X.shape}")
                
                # åŠ è½½æ¨¡å‹
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                
                if model_type == "multiscale":
                    model = MultiScaleTransformerClassifier(
                        input_dim=features.shape[1],
                        model_dim=128,
                        num_heads=8,
                        num_layers=4,
                        num_classes=5,  # 0,1,2,3,4
                        seq_lengths=[10, 30, 60],
                        dropout=0.1
                    ).to(device)
                else:
                    model = TransformerClassifier(
                        input_dim=features.shape[1],
                        model_dim=128,
                        num_heads=8,
                        num_layers=4,
                        num_classes=5,  # 0,1,2,3,4
                        dropout=0.1
                    ).to(device)
                
                model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))
                model.eval()
                
                # è¿›è¡Œé¢„æµ‹ï¼ˆå–å‰100ä¸ªæ ·æœ¬æµ‹è¯•ï¼‰
                test_samples = min(100, len(X))
                X_test = X[:test_samples]
                
                with torch.no_grad():
                    if model_type == "multiscale":
                        # ä¸ºå¤šå°ºåº¦æ¨¡å‹å‡†å¤‡æ•°æ®
                        X_tensors = {}
                        seq_lengths = [10, 30, 60]
                        for seq_len in seq_lengths:
                            X_seq = []
                            for i in range(len(X_test)):
                                if seq_len <= X_test.shape[1]:
                                    X_seq.append(X_test[i][:seq_len])
                                else:
                                    # å¦‚æœåºåˆ—é•¿åº¦ä¸å¤Ÿï¼Œé‡å¤æœ€åä¸€ä¸ªå€¼
                                    seq_data = X_test[i]
                                    while len(seq_data) < seq_len:
                                        seq_data = np.vstack([seq_data, seq_data[-1:]])
                                    X_seq.append(seq_data[:seq_len])
                            X_tensors[str(seq_len)] = torch.tensor(np.array(X_seq), dtype=torch.float32).to(device)
                        
                        outputs = model(X_tensors)
                    else:
                        X_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
                        outputs = model(X_tensor)
                    
                    # è·å–é¢„æµ‹ç»“æœ
                    probs = torch.nn.functional.softmax(outputs, dim=1)
                    predictions = torch.argmax(outputs, dim=1).cpu().numpy()
                    probabilities = probs.cpu().numpy()
                
                # åˆ†æé¢„æµ‹ç»“æœ
                pred_counts = Counter(predictions)
                total_preds = len(predictions)
                signal_count = sum(count for label, count in pred_counts.items() if label != 0)
                prediction_signal_density = signal_count / total_preds
                
                print(f"   âœ… é¢„æµ‹æµ‹è¯•å®Œæˆ")
                print(f"   é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ: {dict(pred_counts)}")
                print(f"   é¢„æµ‹ä¿¡å·å¯†åº¦: {prediction_signal_density:.4f} ({prediction_signal_density*100:.2f}%)")
                
                # åˆ†æç½®ä¿¡åº¦
                max_probs = np.max(probabilities, axis=1)
                avg_confidence = np.mean(max_probs)
                print(f"   å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                
                # è¯Šæ–­ç»“æœ
                print(f"\nğŸ“‹ è¯Šæ–­ç»“æœ:")
                issues = []
                
                if prediction_signal_density < 0.001:
                    issues.append("âŒ é¢„æµ‹ä¿¡å·å¯†åº¦æä½ (<0.1%)")
                    print(f"   é—®é¢˜1: é¢„æµ‹å‡ ä¹æ²¡æœ‰äº§ç”Ÿä»»ä½•äº¤æ˜“ä¿¡å·")
                    
                if avg_confidence < 0.4:
                    issues.append("âš ï¸  é¢„æµ‹ç½®ä¿¡åº¦è¿‡ä½")
                    print(f"   é—®é¢˜2: æ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦ä¸è¶³ï¼Œå¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒ")
                
                if 1 not in pred_counts and 3 not in pred_counts:
                    issues.append("âŒ æ²¡æœ‰å¼€ä»“ä¿¡å·")
                    print(f"   é—®é¢˜3: æ¨¡å‹æ²¡æœ‰é¢„æµ‹å‡ºä»»ä½•å¼€ä»“ä¿¡å·")
                
                if 2 not in pred_counts and 4 not in pred_counts:
                    issues.append("âŒ æ²¡æœ‰å¹³ä»“ä¿¡å·")
                    print(f"   é—®é¢˜4: æ¨¡å‹æ²¡æœ‰é¢„æµ‹å‡ºä»»ä½•å¹³ä»“ä¿¡å·")
                
                # ç»™å‡ºè§£å†³å»ºè®®
                print(f"\nğŸ’¡ è§£å†³å»ºè®®:")
                if prediction_signal_density < 0.001:
                    print(f"   1. ğŸ¯ é™ä½é¢„æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆåœ¨predict_improved.pyä¸­è°ƒæ•´ï¼‰")
                    print(f"   2. ğŸ”„ ä½¿ç”¨å®½æ¾æ ‡ç­¾æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹")
                    print(f"   3. ğŸ“Š æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½äº†å®½æ¾æ ‡ç­¾è®­ç»ƒçš„æƒé‡")
                
                if avg_confidence < 0.4:
                    print(f"   4. ğŸš€ é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå¢åŠ è®­ç»ƒè½®æ•°")
                    print(f"   5. ğŸ›ï¸  è°ƒæ•´æ¨¡å‹è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ¨¡å‹å¤§å°ç­‰ï¼‰")
                
                if len(issues) == 0:
                    print(f"   âœ… é¢„æµ‹åŠŸèƒ½æ­£å¸¸ï¼Œå¯èƒ½æ˜¯å¯è§†åŒ–æˆ–ä¿¡å·è¿‡æ»¤çš„é—®é¢˜")
                    print(f"   å»ºè®®æ£€æŸ¥ plot_improved_signals å‡½æ•°")
                
                return True
            else:
                print(f"   âŒ æ•°æ®é•¿åº¦ä¸è¶³ä»¥åˆ›å»ºåºåˆ—")
                return False
                
        except Exception as e:
            print(f"   âŒ é¢„æµ‹æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    else:
        print(f"   âŒ æ— æ³•è¿›è¡Œé¢„æµ‹æµ‹è¯•ï¼ˆæ•°æ®æˆ–æ¨¡å‹é—®é¢˜ï¼‰")
        return False

def quick_prediction_with_low_threshold():
    """
    ä½¿ç”¨ä½é˜ˆå€¼å¿«é€Ÿé¢„æµ‹æµ‹è¯•
    """
    print(f"\nğŸ§ª ä½é˜ˆå€¼é¢„æµ‹æµ‹è¯•...")
    
    try:
        from predict_improved import main_improved
        
        # æ‰¾ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
        relaxed_dir = "./data_with_relaxed_labels/"
        csv_files = glob.glob(os.path.join(relaxed_dir, "*.csv"))
        
        if csv_files:
            test_file = csv_files[0]
            output_file = "./test_low_threshold_prediction.png"
            
            print(f"   ä½¿ç”¨æ–‡ä»¶: {os.path.basename(test_file)}")
            
            # ä¸´æ—¶ä¿®æ”¹ç½®ä¿¡åº¦é˜ˆå€¼è¿›è¡Œæµ‹è¯•
            results = main_improved(test_file, use_multiscale=True, output_filename=output_file)
            
            if results:
                print(f"   âœ… ä½é˜ˆå€¼é¢„æµ‹å®Œæˆ")
                print(f"   ä¿¡å·å¯†åº¦: {results.get('signal_density', 0):.4f}")
                print(f"   å¹³å‡ç½®ä¿¡åº¦: {results.get('avg_confidence', 0):.3f}")
                
                if results.get('signal_density', 0) > 0.001:
                    print(f"   ğŸ¯ å»ºè®®ï¼šé¢„æµ‹åŠŸèƒ½æ­£å¸¸ï¼Œè°ƒæ•´å¯è§†åŒ–å‚æ•°")
                else:
                    print(f"   ğŸ”§ å»ºè®®ï¼šéœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–è°ƒæ•´æ¨¡å‹å‚æ•°")
            else:
                print(f"   âŒ é¢„æµ‹å¤±è´¥")
        else:
            print(f"   âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ–‡ä»¶")
            
    except Exception as e:
        print(f"   âŒ ä½é˜ˆå€¼é¢„æµ‹æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    success = diagnose_prediction_issue()
    
    if success:
        print(f"\n" + "="*60)
        print(f"ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:")
        print(f"   1. å¦‚æœé¢„æµ‹ä¿¡å·å¯†åº¦è¿‡ä½ï¼Œè€ƒè™‘é‡æ–°è®­ç»ƒæ¨¡å‹")
        print(f"   2. è°ƒæ•´ predict_improved.py ä¸­çš„ç½®ä¿¡åº¦é˜ˆå€¼")
        print(f"   3. æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„å®½æ¾æ ‡ç­¾æ•°æ®è®­ç»ƒ")
        print(f"   4. è¿è¡Œ quick_prediction_with_low_threshold() è¿›è¡Œä½é˜ˆå€¼æµ‹è¯•")
    else:
        print(f"\nâŒ è¯Šæ–­æœªèƒ½å®Œæˆï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œæ¨¡å‹æ–‡ä»¶")