#!/usr/bin/env python3
"""
è¶…ä½ç½®ä¿¡åº¦é¢„æµ‹æµ‹è¯• - å¼ºåˆ¶äº§ç”Ÿäº¤æ˜“ä¿¡å·
ç”¨äºéªŒè¯æ¨¡å‹æ˜¯å¦èƒ½äº§ç”Ÿéé›¶é¢„æµ‹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from predict_fixed import *

def ultra_low_threshold_test(csv_file):
    """
    ä½¿ç”¨è¶…ä½ç½®ä¿¡åº¦é˜ˆå€¼è¿›è¡Œæµ‹è¯•
    """
    print("ğŸ§ª è¶…ä½ç½®ä¿¡åº¦æµ‹è¯•å¼€å§‹...")
    print(f"ç›®æ ‡: å¼ºåˆ¶äº§ç”Ÿäº¤æ˜“ä¿¡å·ï¼ŒéªŒè¯æ¨¡å‹é¢„æµ‹èƒ½åŠ›")
    
    try:
        # 1. åŠ è½½æ•°æ®å’Œæ¨¡å‹
        features, labels, index_values, feature_cols = load_and_preprocess_data_fixed(csv_file)
        model, use_multiscale = load_model_fixed(MODEL_PATH, input_dim=features.shape[1])
        X = create_sequences_fixed(features)
        
        # 2. è·å–åŸå§‹é¢„æµ‹ï¼ˆæ— è¿‡æ»¤ï¼‰
        print("\nğŸ” è·å–åŸå§‹æ¨¡å‹è¾“å‡º...")
        with torch.no_grad():
            X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)
            outputs = model(X_tensor)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            raw_preds = torch.argmax(outputs, dim=1).cpu().numpy()
            probabilities = probs.cpu().numpy()
        
        # 3. è¯¦ç»†åˆ†æåŸå§‹é¢„æµ‹
        print(f"\nğŸ“Š åŸå§‹é¢„æµ‹è¯¦ç»†åˆ†æ:")
        raw_counts = Counter(raw_preds)
        for label in range(5):
            count = raw_counts.get(label, 0)
            percentage = count / len(raw_preds) * 100
            print(f"   æ ‡ç­¾{label}: {count} ä¸ª ({percentage:.1f}%)")
        
        # 4. åˆ†ææ¦‚ç‡åˆ†å¸ƒ
        print(f"\nğŸ² æ¦‚ç‡åˆ†å¸ƒåˆ†æ:")
        for label in range(5):
            label_probs = probabilities[:, label]
            print(f"   æ ‡ç­¾{label} - æœ€å¤§æ¦‚ç‡: {np.max(label_probs):.4f}, å¹³å‡æ¦‚ç‡: {np.mean(label_probs):.4f}")
        
        # 5. å¯»æ‰¾æœ€æœ‰å¯èƒ½çš„éé›¶é¢„æµ‹
        print(f"\nğŸ” å¯»æ‰¾æ½œåœ¨äº¤æ˜“ä¿¡å·...")
        
        # æ‰¾åˆ°æ¯ä¸ªç±»åˆ«æ¦‚ç‡æœ€é«˜çš„æ ·æœ¬
        for label in range(1, 5):  # è·³è¿‡æ— æ“ä½œ(0)
            max_prob_idx = np.argmax(probabilities[:, label])
            max_prob = probabilities[max_prob_idx, label]
            print(f"   æ ‡ç­¾{label}æœ€é«˜æ¦‚ç‡: {max_prob:.4f} (ä½ç½®{max_prob_idx})")
        
        # 6. ä½¿ç”¨æä½é˜ˆå€¼å¼ºåˆ¶äº§ç”Ÿä¿¡å·
        print(f"\nâš¡ ä½¿ç”¨æä½é˜ˆå€¼å¼ºåˆ¶äº§ç”Ÿä¿¡å·...")
        thresholds = [0.05, 0.02, 0.01, 0.001]
        
        for threshold in thresholds:
            # é‡æ–°é¢„æµ‹å¹¶ç»Ÿè®¡
            predictions = raw_preds.copy()
            max_probs = np.max(probabilities, axis=1)
            
            # åªè¿‡æ»¤æä½ç½®ä¿¡åº¦
            low_confidence_mask = max_probs < threshold
            predictions[low_confidence_mask] = 0
            
            signal_count = np.sum(predictions != 0)
            signal_density = signal_count / len(predictions)
            
            print(f"   é˜ˆå€¼{threshold}: {signal_count}ä¸ªä¿¡å· ({signal_density:.4f})")
            
            if signal_count > 0:
                print(f"   âœ… å‘ç°ä¿¡å·! ä½¿ç”¨é˜ˆå€¼{threshold}")
                
                # è¯¦ç»†åˆ†æè¿™äº›ä¿¡å·
                filtered_counts = Counter(predictions)
                for label in range(1, 5):
                    count = filtered_counts.get(label, 0)
                    if count > 0:
                        print(f"     æ ‡ç­¾{label}: {count}ä¸ª")
                
                return predictions, probabilities, threshold
        
        print(f"   âŒ å³ä½¿ä½¿ç”¨æœ€ä½é˜ˆå€¼ä¹Ÿæ— æ³•äº§ç”Ÿä¿¡å·")
        return None, None, None
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None, None, None

def analyze_model_bias(csv_file):
    """
    åˆ†ææ¨¡å‹åå‘æ€§é—®é¢˜
    """
    print(f"\nğŸ”¬ æ¨¡å‹åå‘æ€§åˆ†æ...")
    
    try:
        features, labels, index_values, feature_cols = load_and_preprocess_data_fixed(csv_file)
        model, use_multiscale = load_model_fixed(MODEL_PATH, input_dim=features.shape[1])
        X = create_sequences_fixed(features)
        
        with torch.no_grad():
            X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)
            outputs = model(X_tensor)
            probabilities = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        
        # åˆ†ææ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ
        print(f"\nğŸ“ˆ å„ç±»åˆ«æ¦‚ç‡ç»Ÿè®¡:")
        label_names = ['æ— æ“ä½œ', 'åšå¤šå¼€ä»“', 'åšå¤šå¹³ä»“', 'åšç©ºå¼€ä»“', 'åšç©ºå¹³ä»“']
        
        for label in range(5):
            probs = probabilities[:, label]
            print(f"   {label_names[label]}:")
            print(f"     å¹³å‡æ¦‚ç‡: {np.mean(probs):.4f}")
            print(f"     æœ€å¤§æ¦‚ç‡: {np.max(probs):.4f}")
            print(f"     æ ‡å‡†å·®: {np.std(probs):.4f}")
            print(f"     >0.5çš„æ ·æœ¬: {np.sum(probs > 0.5)}ä¸ª")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æç«¯åå‘
        avg_probs = np.mean(probabilities, axis=0)
        max_avg_prob = np.max(avg_probs)
        max_label = np.argmax(avg_probs)
        
        print(f"\nâš–ï¸  åå‘æ€§æ£€æŸ¥:")
        print(f"   æœ€åå‘çš„ç±»åˆ«: {label_names[max_label]} (å¹³å‡æ¦‚ç‡: {max_avg_prob:.4f})")
        
        if max_avg_prob > 0.8:
            print(f"   âŒ å‘ç°ä¸¥é‡åå‘! æ¨¡å‹è¿‡åº¦åå‘'{label_names[max_label]}'")
        elif max_avg_prob > 0.6:
            print(f"   âš ï¸  å‘ç°ä¸­åº¦åå‘")
        else:
            print(f"   âœ… æ¨¡å‹åå‘æ€§åœ¨åˆç†èŒƒå›´å†…")
        
        return avg_probs
        
    except Exception as e:
        print(f"âŒ åå‘æ€§åˆ†æå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    print("ğŸ§ª è¶…ä½ç½®ä¿¡åº¦é¢„æµ‹æµ‹è¯•")
    print("="*50)
    
    # ä½¿ç”¨å®½æ¾æ ‡ç­¾æ•°æ®è¿›è¡Œæµ‹è¯•
    test_file = "./data_with_relaxed_labels/240110.csv"
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        exit(1)
    
    print(f"ğŸ¯ æµ‹è¯•æ–‡ä»¶: {os.path.basename(test_file)}")
    
    # 1. è¶…ä½ç½®ä¿¡åº¦æµ‹è¯•
    predictions, probabilities, best_threshold = ultra_low_threshold_test(test_file)
    
    # 2. æ¨¡å‹åå‘æ€§åˆ†æ
    bias_analysis = analyze_model_bias(test_file)
    
    # 3. æ€»ç»“å’Œå»ºè®®
    print(f"\n" + "="*60)
    print(f"ğŸ¯ æµ‹è¯•æ€»ç»“å’Œå»ºè®®")
    print(f"="*60)
    
    if predictions is not None:
        print(f"âœ… æˆåŠŸäº§ç”Ÿäº¤æ˜“ä¿¡å·!")
        print(f"   æœ€ä½³é˜ˆå€¼: {best_threshold}")
        print(f"   å»ºè®®: åœ¨å®é™…é¢„æµ‹ä¸­ä½¿ç”¨æ­¤é˜ˆå€¼")
    else:
        print(f"âŒ æ— æ³•äº§ç”Ÿä»»ä½•äº¤æ˜“ä¿¡å·!")
        print(f"   é—®é¢˜: æ¨¡å‹å¯èƒ½è®­ç»ƒè¿‡åº¦ä¿å®ˆ")
        print(f"   å»ºè®®:")
        print(f"     1. ğŸ”„ ä½¿ç”¨å®½æ¾æ ‡ç­¾æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹")
        print(f"     2. ğŸ“Š è°ƒæ•´è®­ç»ƒæ—¶çš„ç±»åˆ«æƒé‡")
        print(f"     3. ğŸ¯ ä½¿ç”¨æˆæœ¬æ•æ„Ÿå­¦ä¹ æ–¹æ³•")
        print(f"     4. âš–ï¸  å¹³è¡¡è®­ç»ƒæ•°æ®ä¸­å„ç±»åˆ«çš„æ¯”ä¾‹")
    
    if bias_analysis is not None:
        max_bias = np.max(bias_analysis)
        if max_bias > 0.8:
            print(f"\nâš ï¸  å‘ç°ä¸¥é‡çš„æ¨¡å‹åå‘é—®é¢˜!")
            print(f"   æ¨¡å‹è¿‡åº¦åå‘'æ— æ“ä½œ'ç±»åˆ«")
            print(f"   å¼ºçƒˆå»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹")
        else:
            print(f"\nğŸ’¡ æ¨¡å‹åå‘æ€§åœ¨å¯æ¥å—èŒƒå›´å†…")
            print(f"   å¯ä»¥å°è¯•è°ƒæ•´é¢„æµ‹é˜ˆå€¼è§£å†³é—®é¢˜")